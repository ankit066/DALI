{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the Dali Pipeline & complatible CUDA version \n",
    "#!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining the Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libararies \n",
    "from nvidia.dali.pipeline import Pipeline\n",
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "\n",
    "#Path of Data \n",
    "image_dir = \"data/images\"\n",
    "max_batch_size = 8\n",
    "\n",
    "#pipeline creation by using decorator \n",
    "@pipeline_def\n",
    "def simple_pipeline():\n",
    "    #Reading the encoded images and labels from hard drive \n",
    "    jpegs, labels = fn.readers.file(file_root=image_dir)\n",
    "    # Decode the images from jpeg to RGB\n",
    "    images = fn.decoders.image(jpegs, device='cpu')\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the pipeline \n",
    "pipe = simple_pipeline(batch_size=max_batch_size, num_threads=1, device_id=0)\n",
    "pipe.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the pipeline.\n",
    "pipe_out = pipe.run()\n",
    "print(pipe_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the TensorList\n",
    "images, labels = pipe_out\n",
    "print(\"Images is_dense_tensor: \" + str(images.is_dense_tensor()))\n",
    "print(\"Labels is_dense_tensor: \" + str(labels.is_dense_tensor()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shapes of images and labels\n",
    "import numpy as np\n",
    "\n",
    "labels_tensor = labels.as_tensor()\n",
    "\n",
    "print (labels_tensor.shape())\n",
    "print (np.array(labels_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the decoded images \n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_images(image_batch):\n",
    "    columns = 4\n",
    "    rows = (max_batch_size + 1) // (columns)\n",
    "    fig = plt.figure(figsize = (32,(32 // columns) * rows))\n",
    "    gs = gridspec.GridSpec(rows, columns)\n",
    "    for j in range(rows*columns):\n",
    "        plt.subplot(gs[j])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(image_batch.at(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Augmentations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Random Suffle to get perfect mix all classes \n",
    "@pipeline_def\n",
    "def shuffled_pipeline():\n",
    "    #intial fill sets capacity of setting this 21 because we have small dataset\n",
    "    jpegs, labels = fn.readers.file(file_root=image_dir, random_shuffle=True, initial_fill=21)\n",
    "    images = fn.decoders.image(jpegs, device='cpu')\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = shuffled_pipeline(batch_size=max_batch_size, num_threads=1, device_id=0, seed=1234)\n",
    "pipe.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_out = pipe.run()\n",
    "images, labels = pipe_out\n",
    "show_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Augmentations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding rotations for obtaining the variety in datasets\n",
    "@pipeline_def\n",
    "def rotated_pipeline():\n",
    "    jpegs, labels = fn.readers.file(file_root=image_dir, random_shuffle=True, initial_fill=21)\n",
    "    images = fn.decoders.image(jpegs, device='cpu')\n",
    "    rotated_images = fn.rotate(images, angle=10.0, fill_value=0)\n",
    "\n",
    "    return rotated_images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that, we added a new operation to our pipeline: `fn.rotate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = rotated_pipeline(batch_size=max_batch_size, num_threads=1, device_id=0, seed=1234)\n",
    "pipe.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_out = pipe.run()\n",
    "images, labels = pipe_out\n",
    "show_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors as Arguments and Random Number Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding random rotations\n",
    "@pipeline_def\n",
    "def random_rotated_pipeline():\n",
    "    jpegs, labels = fn.readers.file(file_root=image_dir, random_shuffle=True, initial_fill=21)\n",
    "    images = fn.decoders.image(jpegs, device='cpu')\n",
    "    angle = fn.random.uniform(range=(-10.0, 10.0))\n",
    "    rotated_images = fn.rotate(images, angle=angle, fill_value=0)\n",
    "\n",
    "    return rotated_images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = random_rotated_pipeline(batch_size=max_batch_size, num_threads=1, device_id=0, seed=1234)\n",
    "pipe.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_out = pipe.run()\n",
    "images, labels = pipe_out\n",
    "show_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying the Tensors to GPU \n",
    "@pipeline_def\n",
    "def random_rotated_gpu_pipeline():\n",
    "    jpegs, labels = fn.readers.file(file_root=image_dir, random_shuffle=True, initial_fill=21)\n",
    "    images = fn.decoders.image(jpegs, device='cpu')\n",
    "    angle = fn.random.uniform(range=(-10.0, 10.0))\n",
    "    #changes operation to images.gpu for copying to GPU\n",
    "    rotated_images = fn.rotate(images.gpu(), angle=angle, fill_value=0)\n",
    "\n",
    "    return rotated_images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = random_rotated_gpu_pipeline(batch_size=max_batch_size, num_threads=1, device_id=0, seed=1234)\n",
    "pipe.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_out = pipe.run()\n",
    "print(pipe_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to access the TensorListGPU for visulaization we need to convert it on cpu.\n",
    "images, labels = pipe_out\n",
    "show_images(images.as_cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed Decoding enables decoding process between CPU and GPU.\n",
    "# %%timeit\n",
    "@pipeline_def\n",
    "def hybrid_pipeline():\n",
    "    jpegs, labels = fn.readers.file(file_root=image_dir, random_shuffle=True, initial_fill=21)\n",
    "    images = fn.decoders.image(jpegs, device='mixed')\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = hybrid_pipeline(batch_size=max_batch_size, num_threads=1, device_id=0, seed=1234)\n",
    "pipe.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_out = pipe.run()\n",
    "images, labels = pipe_out\n",
    "show_images(images.as_cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and Checking time required for CPU and GPU Processing .\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "test_batch_size = 64\n",
    "\n",
    "def speedtest(pipeline, batch, n_threads):\n",
    "    pipe = pipeline(batch_size=batch, num_threads=n_threads, device_id=0)\n",
    "    pipe.build()\n",
    "    # warmup\n",
    "    for i in range(5):\n",
    "        pipe.run()\n",
    "    # test\n",
    "    n_test = 20\n",
    "    t_start = timer()\n",
    "    for i in range(n_test):\n",
    "        pipe.run()\n",
    "    t = timer() - t_start\n",
    "    print(\"Speed: {} imgs/s\".format((n_test * batch)/t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time taken on CPU.\n",
    "speedtest(shuffled_pipeline, test_batch_size, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time taken on GPU.\n",
    "speedtest(hybrid_pipeline, test_batch_size, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using GPU accelerated decoding resulted in significant speedup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
